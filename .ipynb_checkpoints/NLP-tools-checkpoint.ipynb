{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP(Natual Language Process) Tools\n",
    "- `nltk`\n",
    "- `scikit-learn.feature_extraction.text`\n",
    "- Korean Language tools\n",
    "  - `koNLPy`\n",
    "  - `Soynlp`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk\n",
    "NLTK is a leading platform for building `Python programs` to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of **text processing libraries for `classification`, `tokenization`, `stemming`, `tagging`, `parsing`, and `semantic reasoning`, wrappers for industrial-strength NLP libraries**, and an active discussion forum.\n",
    "\n",
    "**Features**:\n",
    " - tokenize\n",
    " - corpus\n",
    " - utils\n",
    " - probability\n",
    " - stem\n",
    " - tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Corpus***\n",
    "\n",
    "A corpus is a `set of sample documents` created for natural language analysis tasks. The `corpus` subpackage of NLTK provides a variety of reasearch corpus. The corpus must be downloaded by the user using the `downlaod` command. The `nltk.download(\"book\")` command downloads most of the corpus required by the NLTK package user manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"book\", quiet=True) # download corpora\n",
    "\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids() # show gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died too long ago for her to have more than an indistinct\n",
      "remembrance of her caresses; and her place had been supplied\n",
      "by an excellent woman as governess, who had fallen little short\n",
      "of a mother in affection.\n",
      "\n",
      "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
      "less as a governess than a friend, very fond of both daughters,\n",
      "but particularly of Emma.  Between _them_ it was more the intimacy\n",
      "of sisters.  Even before Miss Taylor had ceased to hold the nominal\n",
      "office of governess, the mildness o\n"
     ]
    }
   ],
   "source": [
    "# \"austen-emma.txt\"\n",
    "emma = nltk.corpus.gutenberg.raw(\"austen-emma.txt\")\n",
    "document = emma[:1000]\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tokenizing***\n",
    "\n",
    "`tokenizing` is for dividing long strings(documents, sentences, etc.) to smaller units for analysis. This unit of string is called `token`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[len of sent_tokens]: 6 \n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print(\"[len of sent_tokens]:\", len(sent_tokenize(document)), \"\\n\")\n",
    "\n",
    "sentence = sent_tokenize(document)[1]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She',\n",
       " 'was',\n",
       " 'the',\n",
       " 'youngest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'two',\n",
       " 'daughters',\n",
       " 'of',\n",
       " 'a',\n",
       " 'most',\n",
       " 'affectionate',\n",
       " ',',\n",
       " 'indulgent',\n",
       " 'father',\n",
       " ';',\n",
       " 'and',\n",
       " 'had',\n",
       " ',',\n",
       " 'in',\n",
       " 'consequence',\n",
       " 'of',\n",
       " 'her',\n",
       " 'sister',\n",
       " \"'s\",\n",
       " 'marriage',\n",
       " ',',\n",
       " 'been',\n",
       " 'mistress',\n",
       " 'of',\n",
       " 'his',\n",
       " 'house',\n",
       " 'from',\n",
       " 'a',\n",
       " 'very',\n",
       " 'early',\n",
       " 'period',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She',\n",
       " 'was',\n",
       " 'the',\n",
       " 'youngest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'two',\n",
       " 'daughters',\n",
       " 'of',\n",
       " 'a',\n",
       " 'most',\n",
       " 'affectionate',\n",
       " 'indulgent',\n",
       " 'father',\n",
       " 'and',\n",
       " 'had',\n",
       " 'in',\n",
       " 'consequence',\n",
       " 'of',\n",
       " 'her',\n",
       " 'sister',\n",
       " 's',\n",
       " 'marriage',\n",
       " 'been',\n",
       " 'mistress',\n",
       " 'of',\n",
       " 'his',\n",
       " 'house',\n",
       " 'from',\n",
       " 'a',\n",
       " 'very',\n",
       " 'early',\n",
       " 'period']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regex tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "retokenize = RegexpTokenizer(\"[\\w]+\") # \"\\w\" means characters(alphabet + digit + _)\n",
    "                                      # \"+\" means 1 or more of the preceding expression\n",
    "retokenize.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Morphological Analysis*** (형태소분석)\n",
    "\n",
    "`Morpheme` is **the smallest unit of speech in linguistics that has a certain meaning**. Usually, natural language processing uses morphemes as tokens.\n",
    "- `stemming`: the method of finding the basic morpheme by removing the suffix but it does not find the original form of the word. (어간추출)\n",
    "- `lemmatizing`: the task of unifying multiple words with the same meaning into a dictionary form. (원형복원)\n",
    "- `POS tagging (Part of speech)`: a word that is divided into grammatical functions, forms, and meanings. (품사태깅)\n",
    "  - NNP: singular proper nouns\n",
    "  - VB: verbs\n",
    "  - VBP: verb present\n",
    "  - TO: to preposition\n",
    "  - NN: noun (single or collective)\n",
    "  - DT: coronary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer   : ['fli', 'fli', 'fli', 'flew', 'flown']\n",
      "Lancaster Stemmer: ['fly', 'fli', 'fly', 'flew', 'flown']\n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "st1 = PorterStemmer()\n",
    "st2 = LancasterStemmer()\n",
    "\n",
    "words = [\"fly\", \"flies\", \"flying\", \"flew\", \"flown\"]\n",
    "\n",
    "print(\"Porter Stemmer   :\", [st1.stem(w) for w in words])\n",
    "print(\"Lancaster Stemmer:\", [st2.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fly', 'fly', 'fly', 'fly', 'fly']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatizing\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "[lm.lemmatize(w, pos=\"v\") for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('She', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('youngest', 'JJS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('two', 'CD'),\n",
       " ('daughters', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('most', 'RBS'),\n",
       " ('affectionate', 'JJ'),\n",
       " (',', ','),\n",
       " ('indulgent', 'JJ'),\n",
       " ('father', 'NN'),\n",
       " (';', ':'),\n",
       " ('and', 'CC'),\n",
       " ('had', 'VBD'),\n",
       " (',', ','),\n",
       " ('in', 'IN'),\n",
       " ('consequence', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('her', 'PRP$'),\n",
       " ('sister', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('marriage', 'NN'),\n",
       " (',', ','),\n",
       " ('been', 'VBN'),\n",
       " ('mistress', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('his', 'PRP$'),\n",
       " ('house', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('very', 'RB'),\n",
       " ('early', 'JJ'),\n",
       " ('period', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos tagging\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "tagged_list = pos_tag(word_tokenize(sentence))\n",
    "tagged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    }
   ],
   "source": [
    "# find description of tags\n",
    "nltk.help.upenn_tagset(\"NNP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['father', 'consequence', 'sister', 'marriage', 'mistress', 'house', 'period']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract \"NN\" words\n",
    "nouns_list = [t[0] for t in tagged_list if t[1] == \"NN\"]\n",
    "nouns_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Stopwords***\n",
    "\n",
    "Stopwords are words that **do not have meaning** in natural language processing. For example articles, prepositions, conjunctions.. (관사, 전치사, 접속사 ..)\n",
    "- articles: a, an, the\n",
    "- prepositions: in, at, on, from, to, for, into, through...\n",
    "- confuntions: until, before, after, when, while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only word tokenize >>>: 38 \n",
      " ['She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'s\", 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.'] \n",
      "\n",
      "stopwords applied  >>>: 20 \n",
      " ['She', 'youngest', 'two', 'daughters', 'affectionate', ',', 'indulgent', 'father', ';', ',', 'consequence', 'sister', \"'s\", 'marriage', ',', 'mistress', 'house', 'early', 'period', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(sentence)\n",
    "new_tokens = [word for word in tokens if word not in eng_stopwords]\n",
    "\n",
    "print(\"only word tokenize >>>:\", len(tokens), \"\\n\", tokens, \"\\n\")\n",
    "print(\"stopwords applied  >>>:\", len(new_tokens), \"\\n\", new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new stopwords result:  15 \n",
      " ['She', 'youngest', 'two', 'daughters', 'affectionate', 'indulgent', 'father', 'consequence', 'sister', \"'s\", 'marriage', 'mistress', 'house', 'early', 'period']\n"
     ]
    }
   ],
   "source": [
    "# add more stopwords: punctuation marks\n",
    "eng_stopwords = set(stopwords.words('english') + [\".\", \",\", \"?\", \"!\", \":\", \";\"])\n",
    "new_new_tokens = [word for word in tokens if word not in eng_stopwords]\n",
    "\n",
    "print(\"new stopwords result: \", len(new_new_tokens), \"\\n\", new_new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ngrams***\n",
    "\n",
    "Ngrams are a method of **constructing contextual information** to creatre Probabilistic Language Models. in other words, it determines how many words to create a context.\n",
    "\n",
    "- unigram\n",
    "  - $P(w_1, w_2, \\ldots, w_m) = \\prod_{i=1}^m P(w_i)$\n",
    "- bigrams\n",
    "  - $P(w_1, w_2, \\ldots, w_m) = P(w_1) \\prod_{i=2}^{m} P(w_{i}\\;|\\; w_{i-1})$\n",
    "- ngrams\n",
    "  - $P(w_1, w_2, \\ldots, w_m) = P(w_1) \\prod_{i=n}^{m} P(w_{i}\\;|\\; w_{i-1}, \\ldots, w_{i-n})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bigram\n",
      "('She', 'was')\n",
      "('was', 'the')\n",
      "('the', 'youngest')\n",
      "('youngest', 'of')\n",
      "('of', 'the')\n",
      "('the', 'two')\n",
      "('two', 'daughters')\n",
      "('daughters', 'of')\n",
      "('of', 'a')\n",
      "\n",
      "trigram\n",
      "('She', 'was', 'the')\n",
      "('was', 'the', 'youngest')\n",
      "('the', 'youngest', 'of')\n",
      "('youngest', 'of', 'the')\n",
      "('of', 'the', 'two')\n",
      "('the', 'two', 'daughters')\n",
      "('two', 'daughters', 'of')\n",
      "('daughters', 'of', 'a')\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "from nltk.util import ngrams\n",
    "\n",
    "bigram = bigrams(tokens[:10])\n",
    "trigram = ngrams(tokens[:10], 3)\n",
    "\n",
    "print(\"\\nbigram\")\n",
    "for t in bigram:\n",
    "    print(t)\n",
    "    \n",
    "print(\"\\ntrigram\")\n",
    "for t in trigram:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Conditional Probability***\n",
    "\n",
    "`ConditionalFreqDist`\n",
    " - measure the frequency of words in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191785 over\n"
     ]
    }
   ],
   "source": [
    "emma_tokens = word_tokenize(emma)\n",
    "print(len(emma_tokens), emma_tokens[2021])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure the frequency of words\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "cfdist = ConditionalFreqDist()\n",
    "\n",
    "for word in emma_tokens:\n",
    "    condition = len(word)\n",
    "    cfdist[condition][word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An equivalent way:\n",
    "cfdist = ConditionalFreqDist((len(word), word) for word in emma_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4844), ('and', 4653), ('was', 2383), ('her', 2360), ('not', 2242)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are most 5 common words with len=3\n",
    "cfdist[3].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('could', 824),\n",
       " ('would', 813),\n",
       " ('which', 552),\n",
       " ('there', 419),\n",
       " ('every', 398)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are most 5 common words with len=3\n",
    "cfdist[5].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make another CFDist using bigram tokens\n",
    "bigram = bigrams(emma_tokens)\n",
    "cfdist = ConditionalFreqDist((t[0], t[1]) for t in bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('was', 110), ('had', 78), ('could', 33), ('is', 28), ('would', 25)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are most 5 common words right after \"She\"\n",
    "cfdist[\"She\"].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"''\", 1157), ('I', 570), ('``', 416), ('She', 413), ('He', 303)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are most 5 common words right after \".\"\n",
    "cfdist[\".\"].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "cfdist[\"She\"].plot(7, title=\"After 'She'\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('was', 110), ('had', 78), ('could', 33), ('is', 28), ('would', 25)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice that \"She\" and \"she\" are different because we didn't have case integration\n",
    "cfdist[\"She\"].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('had', 254), ('was', 218), ('could', 138), ('is', 94), ('would', 69)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfdist[\"she\"].most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ConditionalProbDist`\n",
    " - estimate conditional probabilities\n",
    " - $P(w\\;|\\; w_c) = \\dfrac{C((w_c, w))}{C((w_c))}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Conditional Probabilities\n",
    "from nltk.probability import ConditionalProbDist, MLEProbDist\n",
    "\n",
    "cpdist = ConditionalProbDist(cfdist, MLEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['was', 'recalled', 'had', 'dearly', 'felt', 'knew', 'would', 'lived', 'loved', 'believed', ',', 'then', 'thought', 'feared', 'is', 'means', 'will', 'inherits', 'knows', 'always', 'thinks', 'played', 'gave', 'must', 'could', 'read', 'paused', 'has', 'desired', 'did', 'cast', 'and', 'ran', 'pondered', 'understood', 'gained', 'hardly', 'shewed', 'hoped', 'allowed', 'heard', 'does', 'should', 'wanted', 'tried', 'looked', 'remembered', 'need', 'might', 'stopt', 'admired', 'now', 'went', 'never', 'got', 'determined', 'often', 'regained', 'certainly', 'seemed', 'seems', 'exerted', 'wished', 'brought', 'meant', 'came', 'opened', 'blessed', 'who', 'plays', 'appeared', 'owned', 'loves', 'followed', 'listened', 'said', 'introduced', 'questioned', 'saw', 'doubted', 'says', 'restrained', 'speaks', 'calls', 'comforted', 'suspected', 'watched', 'liked', 'hesitated', 'depended', 'held', 'merely', 'spoke', 'promised', 'joined', 'smiled', 'laughed', 'still', 'recovered', 'continued', 'resolved', 'asked', 'stopped', 'met', 'touched', 'wrote', 'talked', 'bitterly', 'longed', 'considered', 'deserves', 'supposed', 'made', 'even', 'rose', 'received', 'may', 'frequently', 'gives', 'absolutely', 'closed', 'soon', 'ought', 'checked', 'coloured', 'hears', 'proved'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the words that follow \"She\"\n",
    "cpdist[\"She\"].samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'was'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpdist[\"She\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11575091575091576"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpdist[\"he\"].prob(\"was\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017793594306049824"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpdist[\"She\"].prob(\"has\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0498220640569395"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpdist[\"She\"].prob(\"is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0035587188612099642"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpdist[\"She\"].prob(\"does\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn Text processing modules\n",
    "\n",
    "[***The Bag of Words representation (BOW)***](https://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation)\n",
    "\n",
    "scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
    "- **tokenizing** strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
    "- **counting** the occurrences of tokens in each document.\n",
    "- **normalizing and weighting** with diminishing importance tokens that occur in the majority of samples / documents.\n",
    "\n",
    "In this scheme, features and samples are defined as follows:\n",
    "- each **individual token *occurrence* frequency** (normalized or not) is treated as a **feature**.\n",
    "- the vector of all the token frequencies for a given **document** is considered a multivariate **sample**.\n",
    "\n",
    "A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n",
    "\n",
    "We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or “Bag of n-grams” representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\n",
    "\n",
    "**Sparsity:**\n",
    "\n",
    "As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are **zeros** (typically more than **99%** of them).\n",
    "\n",
    "For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.\n",
    "\n",
    "\n",
    "**Features**:\n",
    " - DictVectorizer\n",
    " - CountVectorizer\n",
    " - TfidfVectorizer\n",
    " - HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DictVectorizer`\n",
    "\n",
    "It inputs the dictionary information indicating the frequency of the occurence of words in the document and convert it into a BOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the fox jumps.',\n",
       " 'the quick fox jumps over.',\n",
       " 'the quick quick brown fox jumps.',\n",
       " 'the some lazy dog.',\n",
       " 'the quick fox fox jumps jumps lazy dog dog']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a document with 4 samples\n",
    "document = \"\"\"the fox jumps.\n",
    "the quick fox jumps over.\n",
    "the quick quick brown fox jumps.\n",
    "the some lazy dog.\n",
    "the quick fox fox jumps jumps lazy dog dog\"\"\"\n",
    "\n",
    "samples = sent_tokenize(document)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the frequency of each word in each sample\n",
    "from collections import Counter\n",
    "\n",
    "token_list = []\n",
    "counts = []\n",
    "for sample in samples:\n",
    "    # tokenize and apply stopwords\n",
    "    tokens = [token for token in word_tokenize(sample) if token not in eng_stopwords]\n",
    "    token_list.append(tokens)\n",
    "    counts.append(Counter(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fox', 'jumps'],\n",
       " ['quick', 'fox', 'jumps'],\n",
       " ['quick', 'quick', 'brown', 'fox', 'jumps'],\n",
       " ['lazy', 'dog'],\n",
       " ['quick', 'fox', 'fox', 'jumps', 'jumps', 'lazy', 'dog', 'dog']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'fox': 1, 'jumps': 1}),\n",
       " Counter({'quick': 1, 'fox': 1, 'jumps': 1}),\n",
       " Counter({'quick': 2, 'brown': 1, 'fox': 1, 'jumps': 1}),\n",
       " Counter({'lazy': 1, 'dog': 1}),\n",
       " Counter({'quick': 1, 'fox': 2, 'jumps': 2, 'lazy': 1, 'dog': 2})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brown', 'dog', 'fox', 'jumps', 'lazy', 'quick']\n",
      "[[0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 1.]\n",
      " [1. 0. 1. 1. 0. 2.]\n",
      " [0. 1. 0. 0. 1. 0.]\n",
      " [0. 2. 2. 2. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "dict_vect = DictVectorizer(sparse=False)\n",
    "X = dict_vect.fit_transform(counts)\n",
    "\n",
    "print(dict_vect.feature_names_)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>jumps</th>\n",
       "      <th>lazy</th>\n",
       "      <th>quick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brown  dog  fox  jumps  lazy  quick\n",
       "0    0.0  0.0  1.0    1.0   0.0    0.0\n",
       "1    0.0  0.0  1.0    1.0   0.0    1.0\n",
       "2    1.0  0.0  1.0    1.0   0.0    2.0\n",
       "3    0.0  1.0  0.0    0.0   1.0    0.0\n",
       "4    0.0  2.0  2.0    2.0   1.0    1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X, columns=dict_vect.feature_names_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.,  0.,  5.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it matches each value to BOW\n",
    "dict_vect.transform({'brown':10, 'fox':5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer`\n",
    "\n",
    "simpler version of `DictVectorizer`\n",
    "\n",
    "1. Convert the document to a token list\n",
    "2. Count the frequency of token occurrence in each document\n",
    "3. Convert each document to a BOW vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the fox jumps.',\n",
       " 'the quick fox jumps over.',\n",
       " 'the quick quick brown fox jumps.',\n",
       " 'the some lazy dog.',\n",
       " 'the quick fox fox jumps jumps lazy dog dog']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brown', 'dog', 'fox', 'jumps', 'lazy', 'quick']\n",
      "[[0 0 1 1 0 0]\n",
      " [0 0 1 1 0 1]\n",
      " [1 0 1 1 0 2]\n",
      " [0 1 0 0 1 0]\n",
      " [0 2 2 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(stop_words=eng_stopwords)\n",
    "count_vect.fit(samples)\n",
    "X = count_vect.transform(samples).toarray()\n",
    "\n",
    "print(count_vect.get_feature_names())\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 5 5 2 4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>jumps</th>\n",
       "      <th>lazy</th>\n",
       "      <th>quick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brown  dog  fox  jumps  lazy  quick\n",
       "0      0    0    1      1     0      0\n",
       "1      0    0    1      1     0      1\n",
       "2      1    0    1      1     0      2\n",
       "3      0    1    0      0     1      0\n",
       "4      0    2    2      2     1      1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.sum(axis=0))\n",
    "pd.DataFrame(X, columns=count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.transform([\"fox jumps again\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.transform([\"big lion runs fast\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown fox</th>\n",
       "      <th>dog dog</th>\n",
       "      <th>fox fox</th>\n",
       "      <th>fox jumps</th>\n",
       "      <th>jumps jumps</th>\n",
       "      <th>jumps lazy</th>\n",
       "      <th>lazy dog</th>\n",
       "      <th>quick brown</th>\n",
       "      <th>quick fox</th>\n",
       "      <th>quick quick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brown fox  dog dog  fox fox  fox jumps  jumps jumps  jumps lazy  lazy dog  \\\n",
       "0          0        0        0          1            0           0         0   \n",
       "1          0        0        0          1            0           0         0   \n",
       "2          1        0        0          1            0           0         0   \n",
       "3          0        0        0          0            0           0         1   \n",
       "4          0        1        1          1            1           1         1   \n",
       "\n",
       "   quick brown  quick fox  quick quick  \n",
       "0            0          0            0  \n",
       "1            0          1            0  \n",
       "2            1          0            1  \n",
       "3            0          0            0  \n",
       "4            0          1            0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ngram_range = (min_n, max_n)\n",
    "# (1, 1) = only unigrams\n",
    "# (1, 2) = unigram & bigram\n",
    "# (2, 2) = only bigrams\n",
    "count_vect2 = CountVectorizer(stop_words=eng_stopwords, ngram_range=(2, 2))\n",
    "X = count_vect2.fit_transform(samples).toarray()\n",
    "\n",
    "pd.DataFrame(X, columns=count_vect2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 5 4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fox</th>\n",
       "      <th>jumps</th>\n",
       "      <th>quick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fox  jumps  quick\n",
       "0    1      1      0\n",
       "1    1      1      1\n",
       "2    1      1      2\n",
       "3    0      0      0\n",
       "4    2      2      1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using \"min_df\", \"max_df\" to limit of word's count\n",
    "count_vect3 = CountVectorizer(min_df=3, stop_words=eng_stopwords)\n",
    "X = count_vect3.fit_transform(samples).toarray()\n",
    "\n",
    "print(X.sum(axis=0))\n",
    "pd.DataFrame(X, columns=count_vect3.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TF-IDF`\n",
    "\n",
    "**TF-IDF(Term Frequency – Inverse Document Frequency) encoding** is a method of reducing weight because it does not count actual the number for words that are commonly contained in all documents. Usually these kind of words lack the ability to distinguish documents.\n",
    "\n",
    "For document $d$ and word $t$:\n",
    "$$\\text{tf-idf}(d, t) = \\text{tf}(d, t) \\cdot \\text{idf}(t)$$\n",
    "\n",
    "- $\\text{tf}(d,t)$: term frequency (=a specific word count)\n",
    "- $n$: samples\n",
    "- $\\text{df}(t)$: the number of document which having a specific word\n",
    "- $\\text{idf}(t)$: inverse document frequency (=inverse of $\\text{df}(t)$)\n",
    " - $\\text{idf}(d, t) = \\log \\dfrac{n}{1 + \\text{df}(t)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>jumps</th>\n",
       "      <th>lazy</th>\n",
       "      <th>over</th>\n",
       "      <th>quick</th>\n",
       "      <th>some</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606856</td>\n",
       "      <td>0.606856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.513275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370649</td>\n",
       "      <td>0.370649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.657899</td>\n",
       "      <td>0.440603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.523001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294649</td>\n",
       "      <td>0.294649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.700519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.507338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.507338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.628833</td>\n",
       "      <td>0.299642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634399</td>\n",
       "      <td>0.442999</td>\n",
       "      <td>0.442999</td>\n",
       "      <td>0.317199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      brown       dog       fox     jumps      lazy      over     quick  \\\n",
       "0  0.000000  0.000000  0.606856  0.606856  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.370649  0.370649  0.000000  0.657899  0.440603   \n",
       "2  0.523001  0.000000  0.294649  0.294649  0.000000  0.000000  0.700519   \n",
       "3  0.000000  0.507338  0.000000  0.000000  0.507338  0.000000  0.000000   \n",
       "4  0.000000  0.634399  0.442999  0.442999  0.317199  0.000000  0.263304   \n",
       "\n",
       "       some       the  \n",
       "0  0.000000  0.513275  \n",
       "1  0.000000  0.313492  \n",
       "2  0.000000  0.249213  \n",
       "3  0.628833  0.299642  \n",
       "4  0.000000  0.187343  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer().fit(samples)\n",
    "X = tfidf_vect.transform(samples).toarray()\n",
    "\n",
    "pd.DataFrame(X, columns=tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>jumps</th>\n",
       "      <th>lazy</th>\n",
       "      <th>over</th>\n",
       "      <th>quick</th>\n",
       "      <th>some</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brown  dog  fox  jumps  lazy  over  quick  some  the\n",
       "0      0    0    1      1     0     0      0     0    1\n",
       "1      0    0    1      1     0     1      1     0    1\n",
       "2      1    0    1      1     0     0      2     0    1\n",
       "3      0    1    0      0     1     0      0     1    1\n",
       "4      0    2    2      2     1     0      1     0    1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare with CountVectorizer\n",
    "X_count = CountVectorizer().fit_transform(samples).toarray()\n",
    "pd.DataFrame(X_count, columns=tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`HashingVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)\n",
    "\n",
    "`CountVectorizer` performs all operations on memory, the larger the size of the document, the slower to execute. `HashingVectorizer` can reduce memory and execution time by using hash functions to generate index for words.\n",
    "\n",
    "This strategy has several advantages:\n",
    "\n",
    "- it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory\n",
    "- it is fast to pickle and un-pickle as it holds no state besides the constructor parameters\n",
    "- it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.\n",
    "\n",
    "There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):\n",
    "\n",
    "- there is no way to compute the inverse transform (from feature indices to string feature names) which can be a problem when trying to introspect which features are most important to a model.\n",
    "- there can be collisions: distinct tokens can be mapped to the same feature index. However in practice this is rarely an issue if n_features is large enough (e.g. 2 ** 18 for text classification problems).\n",
    "- no IDF weighting as this would render the transformer stateful.\n",
    "\n",
    "Param:\n",
    "- `n_features`: The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hash_vect = HashingVectorizer(n_features=4) # n_features: default=2**10\n",
    "X = hash_vect.fit_transform(samples)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.57735027, -0.57735027, -0.57735027],\n",
       "       [ 0.        ,  0.89442719, -0.4472136 ,  0.        ],\n",
       "       [ 0.28867513,  0.8660254 , -0.28867513, -0.28867513],\n",
       "       [ 0.5       , -0.5       , -0.5       , -0.5       ],\n",
       "       [ 0.        ,  0.30151134, -0.30151134, -0.90453403]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty = fetch_20newsgroups()\n",
    "len(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(twenty.data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.64 s, sys: 7.88 ms, total: 1.65 s\n",
      "Wall time: 1.65 s\n"
     ]
    }
   ],
   "source": [
    "%time countvect = CountVectorizer().fit_transform(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.38 s, sys: 3.73 ms, total: 1.38 s\n",
      "Wall time: 1.38 s\n"
     ]
    }
   ],
   "source": [
    "%time hashvec = HashingVectorizer(n_features=2**10).fit_transform(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1024)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashvec.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
